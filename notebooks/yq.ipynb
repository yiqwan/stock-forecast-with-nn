{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Finance\n",
    "import mplfinance as mpf\n",
    "import ta\n",
    "import yfinance as yf\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# System\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('../') # Change the python path at runtime\n",
    "from src.utils import path as path_yq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "cur_dir = Path.cwd()\n",
    "POLYGON_API_KEY = os.environ.get(\"POLYGON_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Tick Data\n",
    "- Add feature to pull from data instead of fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale this up to n years\n",
    "ticker = \"NVDA\"\n",
    "max_limit = 50000\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "api_url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?adjusted=true&sort=asc&limit={max_limit}&apiKey={POLYGON_API_KEY}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"N\" # Reset\n",
    "str = input(\"Confirm?\")\n",
    "\n",
    "if str == \"Y\":\n",
    "    resp = requests.get(api_url)\n",
    "    print(f\"Request made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"VWAP\", \"Transactions\"]\n",
    "if resp.status_code == 200:\n",
    "    dict_list = resp.json().get('results')\n",
    "    df = pd.DataFrame(dict_list)\n",
    "\n",
    "    column_map = {\n",
    "        't': 'Timestamp',\n",
    "        'o': 'Open',\n",
    "        'h': 'High',\n",
    "        'l': 'Low',\n",
    "        'c': 'Close', \n",
    "        'n': 'Transactions', # Number of trades (market activity)\n",
    "        'v': 'Volume', # Number of shares traded (intensity of the activity)\n",
    "        'vw': 'VWAP'\n",
    "    }\n",
    "\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    df['Datetime'] = pd.to_datetime(df['Timestamp'], unit='ms')\n",
    "    df['Date'] = df['Datetime'].dt.normalize() # Remove the time and return date object\n",
    "\n",
    "    df = df[cols]\n",
    "    df.set_index(keys=\"Date\", inplace=True)\n",
    "else:\n",
    "    print(f\"Error fetching data: {resp.status_code}, {resp.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "historical_start_date = '2021-01-01'\n",
    "# Define the ticker list\n",
    "# Excluded 'MXNU' because very weird data\n",
    "ticker_list = ['O5RU', 'BMGU', 'A17U', 'AU8U', 'C38U', 'CWBU', 'CWCU', 'DHLU', 'DCRU', 'BWCU',  'J91U', 'AW9U', 'J69U',\n",
    "               'BUOU', '8U7U', 'UD1U', 'CMOU', 'AJBU', 'K71U', 'JYEU', 'D5IU', 'BTOU', 'ME8U', 'M44U', 'N2IU', 'TS0U', 'SK6U',\n",
    "               'C2PU', 'OXMU', 'M1GU', 'CRPU', 'P40U', 'T82U', 'ODBU']\n",
    "ticker_list = list(map(lambda x: x+'.SI', ticker_list))\n",
    "\n",
    "# Fetch the data\n",
    "data = yf.download(ticker_list, historical_start_date)['Adj Close'] # Auto adjust is false\n",
    "data.index = pd.to_datetime(data.index)\n",
    "# display(data.tail(20))\n",
    "data.plot()\n",
    "\n",
    "plt.figure(figsize=(30,16))\n",
    "# sns.heatmap(data.corr(), cmap=\"Reds\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "historical_start_date_obj = dt.strptime(historical_start_date, '%Y-%m-%d')\n",
    "\n",
    "reits = ['A17U.SI', 'AJBU.SI', 'AU8U.SI', 'AW9U.SI', 'BMGU.SI', 'BTOU.SI',\n",
    "           'BUOU.SI', 'BWCU.SI', 'C2PU.SI', 'C38U.SI', 'CMOU.SI', 'CRPU.SI',\n",
    "           'CWBU.SI', 'D5IU.SI', 'J69U.SI', 'J91U.SI', 'JYEU.SI', 'K71U.SI',\n",
    "           'M1GU.SI', 'M44U.SI', 'ME8U.SI', 'N2IU.SI', 'O5RU.SI', 'OXMU.SI',\n",
    "           'P40U.SI', 'SK6U.SI', 'T82U.SI', 'TS0U.SI', 'UD1U.SI']\n",
    "\n",
    "def get_reit_data(reit_code):\n",
    "    t = yf.Ticker(reit_code)\n",
    "    div = t.dividends\n",
    "    income_stmt = t.income_stmt\n",
    "    \n",
    "    daily_price = t.history(start=historical_start_date)\n",
    "    daily_price = daily_price.drop(columns=['Dividends', 'Stock Splits'])\n",
    "    \n",
    "    # Make the first dividend 0 if no dividends were paid\n",
    "    if str(list(div.items())[0][0]) > str(historical_start_date_obj.year):\n",
    "        first = pd.Series([0], index=[pd.Timestamp(str(list(daily_price.index)[0]))], name=div.name)\n",
    "        div = pd.concat([first, div])\n",
    "    \n",
    "    # Remove up until just before historical_start_date\n",
    "    while str(list(div.items())[1][0]) < str(historical_start_date_obj.year):\n",
    "        div = div[1:]\n",
    "        \n",
    "    indices = []\n",
    "    for i in div.index:\n",
    "        try:\n",
    "            entries = daily_price.loc[i]\n",
    "            indices.append(i)\n",
    "        except KeyError:\n",
    "            first_entry = daily_price.loc[i:]\n",
    "            indices.append(first_entry.iloc[0].name)\n",
    "    div.index = indices\n",
    "    div.index.name = 'Date'\n",
    "    \n",
    "    merged = pd.merge(daily_price, div, left_index=True, right_index=True, how=\"left\").ffill()\n",
    "    merged = merged.rename(columns={'Open': reit_code+'_Open',\n",
    "                                'High': reit_code+'_High', \n",
    "                                'Low': reit_code+'_Low', \n",
    "                                'Close': reit_code+'_Close', \n",
    "                                'Volume': reit_code+'_Volume', \n",
    "                                'Dividends': reit_code+'_Dividends'})\n",
    "    \n",
    "    # Merge EPS to DF\n",
    "    EPS = income_stmt.loc['Basic EPS']\n",
    "    EPS.index = EPS.index.tz_localize('Asia/Singapore')\n",
    "    EPS = EPS.drop(EPS.index[0])\n",
    "    EPS.index.name = 'Date'\n",
    "    \n",
    "    indices = []\n",
    "    for i in EPS.index:\n",
    "        try:\n",
    "            entries = daily_price.loc[i]\n",
    "            indices.append(i)\n",
    "        except KeyError:\n",
    "            first_entry = daily_price.loc[i:]\n",
    "            indices.append(first_entry.iloc[0].name)\n",
    "    EPS.index = indices\n",
    "    \n",
    "    merged = pd.merge(merged, EPS, left_index=True, right_index=True, how=\"left\").ffill()\n",
    "    merged = merged.rename(columns={'Basic EPS': reit_code+'_EPS'})\n",
    "    \n",
    "    # Add P/E ratio to DF\n",
    "    merged[reit_code + '_P/E_Ratio'] = merged[reit_code + '_Close'] / merged[reit_code + '_EPS']\n",
    "    \n",
    "    # Add dividend yield to DF\n",
    "    merged[reit_code + '_Dividend_Yield'] = merged[reit_code + '_Dividends'] / merged[reit_code + '_Close']\n",
    "    \n",
    "    # Add Year, Month, Day\n",
    "    merged.index = pd.to_datetime(merged.index)\n",
    "    merged[reit_code + '_Year'] = merged.index.year\n",
    "    merged[reit_code + '_Month'] = merged.index.month\n",
    "    merged[reit_code + '_Day'] = merged.index.day\n",
    "    \n",
    "    # Change index from datetime to number\n",
    "    merged.index = range(len(merged))\n",
    "    \n",
    "    merged[reit_code + '_Next_Close'] = merged[reit_code + '_Close'].shift(-1)\n",
    "    return merged\n",
    "\n",
    "df = get_reit_data(reits[0])\n",
    "df = df.drop(df.index[-1])\n",
    "df.tail()\n",
    "\n",
    "df.to_csv('new.csv')\n",
    "\n",
    "# Check if the file is created successfully\n",
    "import os\n",
    "print(\"File exists:\", os.path.exists('filename.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = path_yq.get_root_dir(cur_dir=cur_dir)\n",
    "csv_path = Path.joinpath(root_dir, \"data\", f\"{ticker}_{start_date}_{end_date}.csv\")\n",
    "\n",
    "# Get df\n",
    "try:\n",
    "    df.to_csv(csv_path)\n",
    "except NameError:\n",
    "    print(f\"df not defined, trying to fetch from csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Convert data\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index(keys=\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpf plot: https://github.com/matplotlib/mplfinance?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpf.plot(df, type='candle', style='charles', figsize=(20, 10), title=\"OHLC Bars for NVDA\", volume=True, show_nontrading=True, mav=(3, 6, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "- Technical indicators\n",
    "- Fundamental indicators\n",
    "- Date features\n",
    "- Holiday indicators etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- Train-test split\n",
    "- Scale data (only fit-transform for train, but not for test)\n",
    "- Decide which to predict. Have open and predict the close for the same day? Have the close for the previous day and predict next open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_indicators(tmp: pd.DataFrame):\n",
    "    df = tmp.copy(deep=True)\n",
    "\n",
    "    # Date features\n",
    "    df['Year'], df['Month'], df['Day'] = df.index.year, df.index.month, df.index.day\n",
    "\n",
    "    # Use the close price to create the indicators\n",
    "    df['BB High'], df['BB Low']= ta.volatility.bollinger_hband(df['Close']), ta.volatility.bollinger_lband(df['Close'])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Takes the past 20 data, including the current one to calculate the mean\n",
    "    df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
    "\n",
    "    df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "    df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "\n",
    "    df['Future Close'] = df['Close'].shift(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = create_technical_indicators(tmp=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(inplace=True)\n",
    "# Removes the row that doesn't have a target also\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time is {elapsed_time:3f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train, X_val, type='standard'):\n",
    "    \"\"\"\n",
    "    Scale based on the train and val/test data with a specific type.\n",
    "    \"\"\"\n",
    "    type = 'standard'\n",
    "    if type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    return X_train_scaled, X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_numpy_torch(arr: np.ndarray) -> torch:\n",
    "    if isinstance(arr, (pd.DataFrame, pd.Series)):\n",
    "        arr = arr.to_numpy()\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        # This does not make a copy, but any changes will affect the original arr\n",
    "        # Alternative: torch.tensor(arr, dtype=torch.float32)\n",
    "        return torch.from_numpy(arr).float().to(device) # Ensure it is a float tensor, and move to device\n",
    "    else:\n",
    "        raise ValueError(f\"The input\\n{arr}\\nis not an ndarray, it is a {type(arr)}.\")\n",
    "\n",
    "def load_data(X, y, batch_size, shuffle):\n",
    "    dataset = TensorDataset(convert_numpy_torch(X),\n",
    "                                convert_numpy_torch(y))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.min_loss = np.inf\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.delta = 0.1 # TODO: Might wanna adjust based on stock volatility\n",
    "\n",
    "    def stop(self, loss: float) -> bool:\n",
    "        if loss < self.min_loss - self.delta:\n",
    "            self.min_loss = loss\n",
    "            # Reset counter\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM self-define ReLU: https://discuss.pytorch.org/t/change-tanh-activation-in-lstm-to-relu/14160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=None, num_layers=None, **kwargs):\n",
    "        super().__init__() # Need bracket for super\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = 1\n",
    "        self.bidir = False # Hardcoded since bidir won't be used for stock price pred\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size, \n",
    "            hidden_size=self.hidden_size, \n",
    "            num_layers=self.num_layers, # Dropout has no effect on single hidden layer\n",
    "            nonlinearity='tanh', # or relu\n",
    "            bias=True,\n",
    "            batch_first=False,\n",
    "            dropout=0.2,\n",
    "            bidirectional=self.bidir, # Can be True for NLP, but will introduce lookahead bias for stock\n",
    "        )\n",
    "\n",
    "        total_features = self.hidden_size * 2 if self.bidir else self.hidden_size\n",
    "        # Use linear layer for fully connected layer to map to 1 column of output\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=total_features,\n",
    "            out_features=self.output_size\n",
    "            )\n",
    "    \n",
    "    # Must override the parent class's forward method\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "\n",
    "        # print(rnn_out.shape) # [batch_size, features]\n",
    "\n",
    "        return self.fc(rnn_out)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=None, num_layers=None, **kwargs):\n",
    "        super().__init__() # Need bracket for super\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = 1\n",
    "        self.bidir = False # Hardcoded since bidir won't be used for stock price pred\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=self.hidden_size, \n",
    "            num_layers=self.num_layers, # Dropout has no effect on single hidden layer\n",
    "            bias=True,\n",
    "            batch_first=False,\n",
    "            dropout=0.2,\n",
    "            bidirectional=self.bidir, # Can be True for NLP, but will introduce lookahead bias for stock\n",
    "        )\n",
    "\n",
    "        total_features = self.hidden_size * 2 if self.bidir else self.hidden_size\n",
    "        # Use linear layer for fully connected layer to map to 1 column of output\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=total_features,\n",
    "            out_features=self.output_size\n",
    "            )\n",
    "    \n",
    "    # Must override the parent class's forward method\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # print(rnn_out.shape) # [batch_size, features]\n",
    "        return self.fc(lstm_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class custom_model():\n",
    "    def __init__(self, type='RNN', lr=None, n_epoch=None, device=None, **kwargs):\n",
    "        self.lr = lr\n",
    "        self.n_epoch = n_epoch\n",
    "        self.input_size = kwargs.get('input_size', None)\n",
    "        self.hidden_size = kwargs.get('hidden_size', 1)\n",
    "        self.num_layers = kwargs.get('num_layers', 1)\n",
    "        self.train_dataloader = kwargs.get('train_dataloader', None)\n",
    "        self.val_dataloader = kwargs.get('val_dataloader', None)\n",
    "        \n",
    "        if type == 'RNN':\n",
    "            self.model = RNN(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.num_layers\n",
    "            ).to(device)\n",
    "        elif type == 'LSTM':\n",
    "            self.model = LSTM(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.num_layers\n",
    "            ).to(device)\n",
    "        self.optimiser = Adam(params=self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss() # Use sqrt for RMSE\n",
    "        self.model_dict = defaultdict(list)\n",
    "        self.early_stopper = EarlyStopper()\n",
    "\n",
    "    def fit(self):\n",
    "        for epoch in range(self.n_epoch):\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "            total_train_samples = 0\n",
    "            total_val_samples = 0\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            for input, target in self.train_dataloader:\n",
    "                self.optimiser.zero_grad() # Resets gradient of the optimised Tensors to None\n",
    "                output = self.model(input)\n",
    "                # print(output.shape, target.shape)\n",
    "                assert output.squeeze(-1).shape == target.shape\n",
    "                # Using RMSE for both train and eval\n",
    "                loss = self.loss_fn(output, target)\n",
    "                # print(input.shape[0])\n",
    "                total_train_loss += loss.item() * input.shape[0] # Tensor operation: get the scalar in a tensor with 1 element\n",
    "                total_train_samples += input.shape[0]\n",
    "\n",
    "                loss.backward() # Compute the gradient of the loss wrt weights, backpropagate\n",
    "                self.optimiser.step() # Takes a step in the direction that reduces the loss, updates params\n",
    "            \n",
    "            self.model.eval() # Disable dropout\n",
    "            with torch.no_grad():\n",
    "                for input, target in self.val_dataloader:\n",
    "                    output = self.model(input)\n",
    "                    loss = self.loss_fn(output, target)\n",
    "                    total_val_loss += loss.item() * input.shape[0]\n",
    "                    total_val_samples += input.shape[0]\n",
    "            \n",
    "            # Calculating and logging metadata\n",
    "            # Add train, val loss to dict for each epoch\n",
    "            avg_train_loss = math.sqrt(total_train_loss / total_train_samples) # Take the mean of MSE for all batches \n",
    "            avg_val_loss = math.sqrt(total_val_loss / total_val_samples)\n",
    "\n",
    "            self.model_dict['train_loss'].append(avg_train_loss)\n",
    "            self.model_dict['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            self.model_dict['time'].append(elapsed_time)\n",
    "            # print(f\"Elapsed time is {elapsed_time:3f}s.\")\n",
    "\n",
    "            if self.early_stopper.stop(loss=avg_val_loss) == True:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "                break\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1} has completed.\")\n",
    "\n",
    "        # After the whole training is completed, we can plot the losses, time and analyse which model is the best \n",
    "        # Train with full set and save the model for testing\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        model_df = pd.DataFrame(self.model_dict)\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(np.arange(1, len(model_df) + 1), model_df[['train_loss']], label='Train Loss')\n",
    "        plt.plot(np.arange(1, len(model_df) + 1), model_df[['val_loss']], label='Validation Loss')\n",
    "        final_train_loss = model_df[['train_loss']].iloc[-1].item()\n",
    "        final_val_loss = model_df[['val_loss']].iloc[-1].item()\n",
    "        plt.axhline(final_train_loss, color='g', linestyle='--', label=f\"{final_train_loss:.2f}\")\n",
    "        plt.axhline(final_val_loss, color='r', linestyle='--', label=f\"{final_val_loss:.2f}\")\n",
    "        plt.title(f\"Train vs Validation Loss\")\n",
    "        plt.xlabel(f\"Epoch\")\n",
    "        plt.ylabel(f\"RMSE Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "- Need to have train, val, test\n",
    "- General requirements:\n",
    "    - Ideally no NA values, otherwise model performance might be affected\n",
    "    - Number of rows TBC, cannot be too little (not enough data to train), and cannot \n",
    "    be too much also (computationally expensive)\n",
    "- Specific requirements:\n",
    "    - df2 (not sure if need date as index yet)\n",
    "    - Features (should be able to have both categorical and numerical)\n",
    "    - Target variable: Future Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need the current Close to predict the next close\n",
    "X = df2.drop(columns=['Future Close'])\n",
    "y = df2['Future Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterdata = iter(train_dataloader)\n",
    "# input, label = next(iterdata)\n",
    "# input.shape\n",
    "# print(len(train_dataloader), input.shape)\n",
    "# # This is len_seq, batch_size, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different training size to perform time series split/expanding window can\n",
    "help understand how the model performs with different subsets of data.\n",
    "\n",
    "If the model performs worse with a larger set of data, the smaller dataset might\n",
    "not be able to generalise the overall trend, which indicates a potential shift\n",
    "in market trends. Additionally, the features are not likely to have a high predictive\n",
    "power on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') # torch.device('cpu')\n",
    "n_splits = 3\n",
    "split_idx = int(len(df2) * 0.8)\n",
    "scaler_type = 'minmax' # Standard scaler does not provide good results\n",
    "batch_size = 32 # Larger batch size takes longer to converge, tried 64 and 32\n",
    "shuffle = False\n",
    "\n",
    "# FIXME: Assess the model on different sets and use the best one\n",
    "# TODO: Gap\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, gap=10)\n",
    "\n",
    "# TODO: Assess the models\n",
    "# Now do CV for assessing robustness, but later on, the train and validation should\n",
    "# Choose the best model/just go according to the sequential split\n",
    "print(f\"Length train + val, test: {split_idx}, {len(df2) - split_idx}\")\n",
    "for train_idx, val_idx in tscv.split(X.iloc[:split_idx]):\n",
    "    print(f\"Length train, val: {len(train_idx)}, {len(val_idx)}.\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    X_train_scaled, X_val_scaled = scale(X_train=X_train, X_val=X_val, type=scaler_type)\n",
    "    \n",
    "    train_dataloader = load_data(X=X_train_scaled, y=y_train, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = load_data(X=X_val_scaled, y=y_val, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    kwargs = {\n",
    "        'input_size': X.shape[-1],\n",
    "        'hidden_size': 32 , # 64 and 128 have weird spikes at the end\n",
    "        'num_layers': 4,\n",
    "        'train_dataloader': train_dataloader,\n",
    "        'val_dataloader': val_dataloader\n",
    "        \n",
    "    }\n",
    "    # lr 0.1 is too high, will fluctuate at the end\n",
    "    # rnn = custom_model(type='RNN', lr=0.05, n_epoch=100, device=device, **kwargs) # It's wrong to write kwargs=kwargs\n",
    "    # rnn.fit()\n",
    "    # rnn.plot_loss()\n",
    "\n",
    "    lstm = custom_model(type='LSTM', lr=0.05, n_epoch=100, device=device, **kwargs) # It's wrong to write kwargs=kwargs\n",
    "    lstm.fit()\n",
    "    lstm.plot_loss()\n",
    "X_test, y_test = X.iloc[split_idx:], y.iloc[split_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add analysis to the lost_dict and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards for really testing against test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X.iloc[:split_idx], y.iloc[:split_idx], shuffle=False, test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
