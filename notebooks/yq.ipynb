{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Finance\n",
    "import mplfinance as mpf\n",
    "import ta\n",
    "import yfinance as yf\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "# System\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('../') # Change the python path at runtime\n",
    "from src.utils import path as path_yq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "cur_dir = Path.cwd()\n",
    "\n",
    "POLYGON_API_KEY = os.environ.get(\"POLYGON_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Tick Data\n",
    "- Add feature to pull from data instead of fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale this up to n years\n",
    "ticker = \"NVDA\"\n",
    "max_limit = 50000\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "api_url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?adjusted=true&sort=asc&limit={max_limit}&apiKey={POLYGON_API_KEY}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"N\" # Reset\n",
    "str = input(\"Confirm?\")\n",
    "\n",
    "if str == \"Y\":\n",
    "    resp = requests.get(api_url)\n",
    "    print(f\"Request made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"VWAP\", \"Transactions\"]\n",
    "if resp.status_code == 200:\n",
    "    dict_list = resp.json().get('results')\n",
    "    df = pd.DataFrame(dict_list)\n",
    "\n",
    "    column_map = {\n",
    "        't': 'Timestamp',\n",
    "        'o': 'Open',\n",
    "        'h': 'High',\n",
    "        'l': 'Low',\n",
    "        'c': 'Close', \n",
    "        'n': 'Transactions', # Number of trades (market activity)\n",
    "        'v': 'Volume', # Number of shares traded (intensity of the activity)\n",
    "        'vw': 'VWAP'\n",
    "    }\n",
    "\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    df['Datetime'] = pd.to_datetime(df['Timestamp'], unit='ms')\n",
    "    df['Date'] = df['Datetime'].dt.normalize() # Remove the time and return date object\n",
    "\n",
    "    df = df[cols]\n",
    "    df.set_index(keys=\"Date\", inplace=True)\n",
    "else:\n",
    "    print(f\"Error fetching data: {resp.status_code}, {resp.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "historical_start_date = '2022-08-09'\n",
    "# Define the ticker list\n",
    "# Capitaland A17U, SUNT.SI cannot download\n",
    "ticker_list = ['S51.SI']\n",
    "\n",
    "# Fetch the data\n",
    "data = yf.download(ticker_list, historical_start_date)['Adj Close'] # Auto adjust is false\n",
    "data.index = pd.to_datetime(data.index)\n",
    "display(data.tail(20))\n",
    "data.plot()\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "sns.heatmap(data.corr(), cmap=\"Reds\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = path_yq.get_root_dir(cur_dir=cur_dir)\n",
    "csv_path = Path.joinpath(root_dir, \"data\", f\"{ticker}_{start_date}_{end_date}.csv\")\n",
    "\n",
    "# Get df\n",
    "try:\n",
    "    df.to_csv(csv_path)\n",
    "except NameError:\n",
    "    print(f\"df not defined, trying to fetch from csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Convert data\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index(keys=\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpf plot: https://github.com/matplotlib/mplfinance?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpf.plot(df, type='candle', style='charles', figsize=(20, 10), title=\"OHLC Bars for NVDA\", volume=True, show_nontrading=True, mav=(3, 6, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Bryan's Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path.joinpath(path_yq.get_root_dir(cur_dir=cur_dir), 'data', 'capitaland.csv')\n",
    "df2 = pd.read_csv(csv_path)\n",
    "df2['Date'] = pd.to_datetime(df2['Date']).dt.normalize()\n",
    "\n",
    "assert (df2.index == df2.sort_values(by='Date', ascending=True).index).all(), 'df is not sorted in ascending'\n",
    "df2['Year'] = df2.Date.dt.year\n",
    "df2['Month'] = df2.Date.dt.month\n",
    "df2['Day'] = df2.Date.dt.day\n",
    "\n",
    "if input('Add additional features?') == 'Y':\n",
    "    # Testing additional features\n",
    "    df2['BB High'], df2['BB Low']= ta.volatility.bollinger_hband(df2['A17U.SI_Close']), ta.volatility.bollinger_lband(df2['A17U.SI_Close'])\n",
    "\n",
    "    df2['MA_50'] = df2['A17U.SI_Close'].rolling(window=50).mean()\n",
    "\n",
    "    df2['MA_20'] = df2['A17U.SI_Close'].rolling(window=20).mean()\n",
    "\n",
    "    df2['MA_5'] = df2['A17U.SI_Close'].rolling(window=5).mean()\n",
    "df2.drop(columns='Date', inplace=True)\n",
    "df2.dropna(inplace=True)\n",
    "print(df2.isna().sum())\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "- Technical indicators\n",
    "- Fundamental indicators\n",
    "- Date features\n",
    "- Holiday indicators etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- Train-test split\n",
    "- Scale data (only fit-transform for train, but not for test)\n",
    "- Decide which to predict. Have open and predict the close for the same day? Have the close for the previous day and predict next open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_indicators(tmp: pd.DataFrame):\n",
    "    df = tmp.copy(deep=True)\n",
    "\n",
    "    # Date features\n",
    "    # df['Year'], df['Month'], df['Day'] = df.index.year, df.index.month, df.index.day\n",
    "    # FIXME: Test no date\n",
    "    # Use the close price to create the indicators\n",
    "    df['BB High'], df['BB Low']= ta.volatility.bollinger_hband(df['Close']), ta.volatility.bollinger_lband(df['Close'])\n",
    "    \n",
    "    # Takes the past 20 data, including the current one to calculate the mean\n",
    "    df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
    "\n",
    "    df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "    df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "\n",
    "    df['Future Close'] = df['Close'].shift(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = create_technical_indicators(tmp=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(inplace=True)\n",
    "# Removes the row that doesn't have a target also\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time is {elapsed_time:3f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    def __init__(self, type='standard') -> None:\n",
    "        self.scaler = None\n",
    "        if type == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif type == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def scale_fit_transform(self, train, val):\n",
    "        train_scaled = self.scaler.fit_transform(train)\n",
    "        val_scaled = self.scaler.transform(val)\n",
    "\n",
    "        return train_scaled, val_scaled\n",
    "\n",
    "    def scale_transform(self, test):\n",
    "        return self.scaler.transform(test)\n",
    "\n",
    "\n",
    "# def scale(X_train, X_val, type='standard'):\n",
    "#     \"\"\"\n",
    "#     Scale based on the train and val/test data with a specific type.\n",
    "#     \"\"\"\n",
    "#     type = 'standard'\n",
    "#     if type == 'standard':\n",
    "#         scaler = StandardScaler()\n",
    "#     else:\n",
    "#         scaler = MinMaxScaler()\n",
    "\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "#     return X_train_scaled, X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy_torch(arr) -> torch:\n",
    "    if isinstance(arr, (pd.DataFrame, pd.Series)):\n",
    "        arr = arr.to_numpy()\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        # This does not make a copy, but any changes will affect the original arr\n",
    "        # Alternative: torch.tensor(arr, dtype=torch.float32)\n",
    "        return torch.from_numpy(arr).float().to(DEVICE) # Ensure it is a float tensor, and move to device\n",
    "    else:\n",
    "        raise ValueError(f\"The input\\n{arr}\\nis not an ndarray, it is a {type(arr)}.\")\n",
    "\n",
    "# Only X needs to be turned into a sequence\n",
    "def load_sequence(X, seq_len):\n",
    "    sequences = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        sequences.append(X[i: i + seq_len])\n",
    "    print(np.array(sequences).shape)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def load_data(X, y, batch_size, shuffle):\n",
    "    dataset = TensorDataset(convert_numpy_torch(X),\n",
    "                                convert_numpy_torch(y))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.min_loss = np.inf\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.delta = 0 # TODO: Might wanna adjust based on stock volatility\n",
    "\n",
    "    def stop(self, loss: float) -> bool:\n",
    "        if loss < self.min_loss - self.delta:\n",
    "            self.min_loss = loss\n",
    "            # Reset counter\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM self-define ReLU: https://discuss.pytorch.org/t/change-tanh-activation-in-lstm-to-relu/14160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=None, num_layers=None, **kwargs):\n",
    "        super().__init__() # Need bracket for super\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = 1\n",
    "        self.bidir = False # Hardcoded since bidir won't be used for stock price pred\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size, \n",
    "            hidden_size=self.hidden_size, \n",
    "            num_layers=self.num_layers, # Dropout has no effect on single hidden layer\n",
    "            nonlinearity='tanh', # or relu\n",
    "            bias=True,\n",
    "            batch_first=False,\n",
    "            dropout=0.2,\n",
    "            bidirectional=self.bidir, # Can be True for NLP, but will introduce lookahead bias for stock\n",
    "        )\n",
    "\n",
    "        total_features = self.hidden_size * 2 if self.bidir else self.hidden_size\n",
    "        # Use linear layer for fully connected layer to map to 1 column of output\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=total_features,\n",
    "            out_features=self.output_size\n",
    "            )\n",
    "    \n",
    "    # Must override the parent class's forward method\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "\n",
    "        # print(rnn_out.shape) # [batch_size, features]\n",
    "\n",
    "        return self.fc(rnn_out)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=None, num_layers=None, **kwargs):\n",
    "        super().__init__() # Need bracket for super\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = 1\n",
    "        self.bidir = False # Hardcoded since bidir won't be used for stock price pred\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=self.hidden_size, \n",
    "            num_layers=self.num_layers, # Dropout has no effect on single hidden layer\n",
    "            bias=True,\n",
    "            batch_first=False,\n",
    "            dropout=0.2,\n",
    "            bidirectional=self.bidir, # Can be True for NLP, but will introduce lookahead bias for stock\n",
    "        )\n",
    "\n",
    "        total_features = self.hidden_size * 2 if self.bidir else self.hidden_size\n",
    "        # Use linear layer for fully connected layer to map to 1 column of output\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=total_features,\n",
    "            out_features=self.output_size\n",
    "            )\n",
    "    \n",
    "    # Must override the parent class's forward method\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # print(rnn_out.shape) # [batch_size, features]\n",
    "        return self.fc(lstm_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "class custom_model():\n",
    "    def __init__(self, type='RNN', lr=None, n_epoch=None, device=None, **kwargs):\n",
    "        self.lr = lr\n",
    "        self.n_epoch = n_epoch\n",
    "        self.input_size = kwargs.get('input_size', None)\n",
    "        self.hidden_size = kwargs.get('hidden_size', 1)\n",
    "        self.num_layers = kwargs.get('num_layers', 1)\n",
    "        self.train_dataloader = kwargs.get('train_dataloader', None)\n",
    "        self.val_dataloader = kwargs.get('val_dataloader', None)\n",
    "        self.scaler_y = kwargs.get('scaler_y', None)\n",
    "        self.device = device\n",
    "        \n",
    "        if type == 'RNN':\n",
    "            self.model = RNN(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.num_layers\n",
    "            ).to(device)\n",
    "        elif type == 'LSTM':\n",
    "            self.model = LSTM(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.num_layers\n",
    "            ).to(device)\n",
    "        self.optimiser = Adam(params=self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss() # Use sqrt for RMSE\n",
    "        self.model_dict = defaultdict(list)\n",
    "        self.early_stopper = EarlyStopper()\n",
    "\n",
    "    def fit(self):\n",
    "        for epoch in range(self.n_epoch):\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "            total_train_samples = 0\n",
    "            total_val_samples = 0\n",
    "            pred_dict = defaultdict(list)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            for input, target in self.train_dataloader:\n",
    "                # input, target = input.to(self.device), target.to(self.device)\n",
    "                self.optimiser.zero_grad() # Resets gradient of the optimised Tensors to None\n",
    "                output = self.model(input)\n",
    "                # print(output.shape, target.shape)\n",
    "                output = output.squeeze(-1)\n",
    "                # assert output.squeeze(-1).shape == target.shape\n",
    "                # Using RMSE for both train and eval\n",
    "                \n",
    "                loss = self.loss_fn(output, target)\n",
    "                # print(input.shape[0])\n",
    "                # print(loss.item())\n",
    "                total_train_loss += loss.item() * input.shape[0] # Tensor operation: get the scalar in a tensor with 1 element\n",
    "                total_train_samples += input.shape[0]\n",
    "\n",
    "                loss.backward() # Compute the gradient of the loss wrt weights, backpropagate\n",
    "\n",
    "                # for name, param in self.model.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(f\"Layer: {name} | Max Gradient: {param.grad.data.abs().max()}\")\n",
    "                self.optimiser.step() # Takes a step in the direction that reduces the loss, updates params\n",
    "            \n",
    "            self.model.eval() # Disable dropout\n",
    "            with torch.no_grad():\n",
    "                for input, target in self.val_dataloader:\n",
    "                    # print(f\"Before to device: Input: {input.device}, target: {target.device}\")\n",
    "                    # input, target = input.to(self.device), target.to(self.device) \n",
    "                    # Manually move to device even though dataloader ady moved\n",
    "                    # Doesn't really make a difference\n",
    "                    output = self.model(input)\n",
    "                    output = output.squeeze(-1)\n",
    "                    loss = self.loss_fn(output, target)\n",
    "                    total_val_loss += loss.item() * input.shape[0]\n",
    "                    total_val_samples += input.shape[0]\n",
    "\n",
    "                    # Not efficient since it's only used after all epochs end\n",
    "                    pred_dict['predicted'].append(self.scaler_y.scaler.inverse_transform(np.reshape(output.tolist(), (-1, 1)))) # Convert tensor to list\n",
    "                    pred_dict['actual'].append(self.scaler_y.scaler.inverse_transform(np.reshape(target.tolist(), (-1, 1))))\n",
    "            \n",
    "            # Calculating and logging metadata\n",
    "            # Add train, val loss to dict for each epoch\n",
    "            avg_train_loss = math.sqrt(total_train_loss / total_train_samples) # Take the mean of MSE for all batches \n",
    "            avg_val_loss = math.sqrt(total_val_loss / total_val_samples)\n",
    "\n",
    "            self.model_dict['train_loss'].append(avg_train_loss)\n",
    "            self.model_dict['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            self.model_dict['time'].append(elapsed_time)\n",
    "            # print(f\"Elapsed time is {elapsed_time:3f}s.\")\n",
    "\n",
    "            if self.early_stopper.stop(loss=avg_val_loss) == True:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "                break\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1} has completed.\")\n",
    "\n",
    "        # Only plot the predicted vs actual for the last epoch\n",
    "        pred_dict['predicted'] = [item for sublist in pred_dict['predicted'] for item in sublist]\n",
    "        pred_dict['actual'] = [item for sublist in pred_dict['actual'] for item in sublist]\n",
    "\n",
    "        self.plot_pred_actl(actual_list=pred_dict['actual'], pred_list=pred_dict['predicted'])\n",
    "\n",
    "        # After the whole training is completed, we can plot the losses, time and analyse which model is the best \n",
    "        # Train with full set and save the model for testing\n",
    "\n",
    "    def evaluate(self, test_dataloader):\n",
    "        \"\"\"\n",
    "        Predict the test dataset that has labels but not used in training phase.\n",
    "        \"\"\"\n",
    "\n",
    "        total_test_loss = 0\n",
    "        total_test_samples = 0\n",
    "        self.model.eval() # Disable dropout\n",
    "\n",
    "        pred_dict = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for input, target in test_dataloader:\n",
    "                output = self.model(input)\n",
    "                output = output.squeeze(-1)\n",
    "                loss = self.loss_fn(output, target)\n",
    "                total_test_loss += loss.item() * input.shape[0]\n",
    "                total_test_samples += input.shape[0]\n",
    "\n",
    "                pred_dict['predicted'].append(self.scaler_y.scaler.inverse_transform(np.reshape(output.tolist(), (-1, 1)))) # Convert tensor to list\n",
    "                pred_dict['actual'].append(self.scaler_y.scaler.inverse_transform(np.reshape(target.tolist(), (-1, 1))))\n",
    "\n",
    "        pred_dict['predicted'] = [item for sublist in pred_dict['predicted'] for item in sublist]\n",
    "        pred_dict['actual'] = [item for sublist in pred_dict['actual'] for item in sublist]\n",
    "        avg_test_loss = math.sqrt(total_test_loss / total_test_samples)\n",
    "\n",
    "        # pred_list = pred_dict['predicted']\n",
    "        # actual_list = pred_dict['actual']\n",
    "\n",
    "        # rmse = root_mean_squared_error(y_true=actual_list, y_pred=pred_list)\n",
    "        # print(f\"RMSE: {rmse:.5f}, average test loss: {avg_test_loss:.5f} should be similar\") \n",
    "\n",
    "        self.plot_pred_actl(pred_list=pred_dict['predicted'], actual_list=pred_dict['actual'])\n",
    "        \n",
    "        return pred_dict, avg_test_loss\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predicts unforeseen data without any evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "    def plot_pred_actl(self, pred_list, actual_list):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x = range(len(pred_list))\n",
    "\n",
    "        plt.plot(x, pred_list, label='Predicted')\n",
    "        plt.plot(x, actual_list, label='Actual')\n",
    "        plt.title('Actual vs Predicted') # TODO: Add in val or test\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        model_df = pd.DataFrame(self.model_dict)\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(np.arange(1, len(model_df) + 1), model_df[['train_loss']], label='Train Loss')\n",
    "        plt.plot(np.arange(1, len(model_df) + 1), model_df[['val_loss']], label='Validation Loss')\n",
    "        final_train_loss = model_df[['train_loss']].iloc[-1].item()\n",
    "        final_val_loss = model_df[['val_loss']].iloc[-1].item()\n",
    "        plt.axhline(final_train_loss, color='g', linestyle='--', label=f\"{final_train_loss:.2f}\")\n",
    "        plt.axhline(final_val_loss, color='r', linestyle='--', label=f\"{final_val_loss:.2f}\")\n",
    "        plt.title(f\"Train vs Validation Loss\")\n",
    "        plt.xlabel(f\"Epoch\")\n",
    "        plt.ylabel(f\"RMSE Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data - testing\n",
    "\n",
    "- Need to have train, val, test\n",
    "- General requirements:\n",
    "    - Ideally no NA values, otherwise model performance might be affected\n",
    "    - Number of rows TBC, cannot be too little (not enough data to train), and cannot \n",
    "    be too much also (computationally expensive)\n",
    "- Specific requirements:\n",
    "    - df2 (not sure if need date as index yet)\n",
    "    - Features (should be able to have both categorical and numerical)\n",
    "    - Target variable: Future Close\n",
    "    - df2 can only contain numerical variables for now, if there are datetime objects\n",
    "    or other types, need to convert to categorical (ColumnTransformer, Pipeline, OneHotEncoder()) and numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need the current Close to predict the next close\n",
    "cfm = 'N'\n",
    "cfm = input('Run Bryan data?')\n",
    "if cfm == 'Y':\n",
    "    target_col_name = 'A17U.SI_Next_Close'\n",
    "    X = df2.drop(columns=[target_col_name])\n",
    "    y = df2[target_col_name]\n",
    "else:\n",
    "    X = df2.drop(columns=['Future Close'])\n",
    "    y = df2['Future Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterdata = iter(train_dataloader)\n",
    "# input, label = next(iterdata)\n",
    "# input.shape\n",
    "# print(len(train_dataloader), input.shape)\n",
    "# # This is len_seq, batch_size, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different training size to perform time series split/expanding window can\n",
    "help understand how the model performs with different subsets of data.\n",
    "\n",
    "If the model performs worse with a larger set of data, the smaller dataset might\n",
    "not be able to generalise the overall trend, which indicates a potential shift\n",
    "in market trends. Additionally, the features are not likely to have a high predictive\n",
    "power on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('mps')\n",
    "device = torch.device('mps') # torch.device('cpu')\n",
    "n_splits = 3\n",
    "split_idx = int(len(df2) * 0.8)\n",
    "scaler_type = 'minmax' # Standard scaler does not provide good results\n",
    "batch_size = 1 # Larger batch size takes longer to converge, tried 64 and 32\n",
    "shuffle = False\n",
    "\n",
    "# FIXME: Assess the model on different sets and use the best one\n",
    "# TODO: Gap\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, gap=10)\n",
    "\n",
    "# TODO: Assess the models\n",
    "# Now do CV for assessing robustness, but later on, the train and validation should\n",
    "# Choose the best model/just go according to the sequential split\n",
    "print(f\"Length train + val, test: {split_idx}, {len(df2) - split_idx}\")\n",
    "for train_idx, val_idx in tscv.split(X.iloc[:split_idx]):\n",
    "    print(f\"Length train, val: {len(train_idx)}, {len(val_idx)}.\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    scaler_x, scaler_y = Scaler(type=scaler_type), Scaler(type=scaler_type)\n",
    "    X_train_scaled, X_val_scaled = scaler_x.scale_fit_transform(train=X_train, val=X_val)\n",
    "    y_train_scaled, y_val_scaled = scaler_y.scale_fit_transform(\n",
    "        train=np.reshape(y_train, (-1, 1)), \n",
    "        val=np.reshape(y_val, (-1, 1)))\n",
    "    \n",
    "    train_dataloader = load_data(X=X_train_scaled, y=y_train_scaled, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = load_data(X=X_val_scaled, y=y_val_scaled, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    kwargs = {\n",
    "        'input_size': X.shape[-1],\n",
    "        'hidden_size': 256 , # 64 and 128 have weird spikes at the end\n",
    "        'num_layers': 16,\n",
    "        'train_dataloader': train_dataloader,\n",
    "        'val_dataloader': val_dataloader,\n",
    "        'scaler_y': scaler_y,\n",
    "        \n",
    "    }\n",
    "    # lr 0.1 is too high, will fluctuate at the end \n",
    "    # FIXME: Edit n_epoch\n",
    "    model = custom_model(type='RNN', lr=0.0001, n_epoch=100, device=DEVICE, **kwargs) # It's wrong to write kwargs=kwargs\n",
    "    model.fit()\n",
    "    model.plot_loss()\n",
    "\n",
    "    # model = custom_model(type='LSTM', lr=0.05, n_epoch=100, device=device, **kwargs) # It's wrong to write kwargs=kwargs\n",
    "    # model.fit()\n",
    "    # model.plot_loss()\n",
    "\n",
    "# Use the last model with all the training and validation data\n",
    "test_model = model\n",
    "X_test, y_test = X.iloc[split_idx:], y.iloc[split_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test another RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Daammon/Stock-Prediction-with-RNN-in-Pytorch/blob/master/Rnn0.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "X = df2.drop(columns=['Future Close'])\n",
    "y = df2['Future Close']\n",
    "\n",
    "# Define the LSTM-based neural network model\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers): # FIXME: input size\n",
    "        super(Model1, self).__init__()\n",
    "        # LSTM layer with specified input_size, hidden_size, and num_layers, batch_first ensures\n",
    "        # that the input and output tensors are provided as (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # Output layer that maps the output of the LSTM to the desired output size\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "        # Batch normalization applied to outputs across the batch dimension\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "    def forward(self, x, h_state, c_state):\n",
    "        # Forward pass through LSTM\n",
    "        r_out, (h_state, c_state) = self.lstm(x, (h_state, c_state))\n",
    "        # Apply batch normalization to the outputs at each time step\n",
    "        # r_out = self.bn(r_out)\n",
    "        \n",
    "        # # Collect the predictions for each time step\n",
    "        # outs = []\n",
    "        # for time_step in range(r_out.size(1)):\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "        \n",
    "        # # Stack predictions into a tensor\n",
    "        # return torch.stack(outs, dim=1), (h_state, c_state)\n",
    "        return self.out(r_out), (h_state, c_state)\n",
    "\n",
    "# Dataset class that handles loading of data\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X_dataset, Y_dataset):\n",
    "        # Convert the input datasets to tensors\n",
    "        self.X_dataset = torch.from_numpy(X_dataset).float()\n",
    "        self.Y_dataset = torch.from_numpy(Y_dataset).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return self.X_dataset.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the sequence and the target value at the given index\n",
    "        sequence = self.X_dataset[idx, :, :]\n",
    "        target = self.Y_dataset[idx]\n",
    "\n",
    "        # print(sequence.shape, target.shape)\n",
    "        return {'Sequence': sequence, 'Target': target}\n",
    "\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 64\n",
    "hidden_size = 20\n",
    "num_layers = 4\n",
    "\n",
    "# Initialize the neural network, transfer it to GPU if available\n",
    "net = Model1(input_size=X.shape[-1], hidden_size=hidden_size, num_layers=num_layers).to(DEVICE)\n",
    "\n",
    "# Initialize the hidden and cell states of LSTM\n",
    "h_state = torch.zeros(num_layers, batch_size, hidden_size).to(DEVICE)\n",
    "c_state = torch.zeros(num_layers, batch_size, hidden_size).to(DEVICE)\n",
    "\n",
    "# Define a loss function and the optimizer \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00005)\n",
    "\n",
    "\n",
    "train_dataset = StockDataset(load_sequence(X, 20), y.to_numpy()) # FIXME: Might have diff length\n",
    "# train_dataset = load_data(load_sequence(X, 20), y, 64, False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    prediction_plot = []\n",
    "\n",
    "    net.train()\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data['Sequence'], data['Target']\n",
    "        \n",
    "        # Wrap inputs and labels in Variables, transfer them to GPU if available\n",
    "        x = Variable(inputs).to(DEVICE)\n",
    "        y = Variable(labels).to(DEVICE)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass, backward pass, and optimize\n",
    "        prediction, (h_state, c_state) = net(x, h_state, c_state)\n",
    "        # Get the predictions for the last time step\n",
    "        # print(prediction)\n",
    "        prediction = prediction[:, 19, :].squeeze() # FIXME: hardcoded\n",
    "        # Detach states from the graph to prevent backpropagation through the entire sequence\n",
    "        h_state = Variable(h_state.data)\n",
    "        c_state = Variable(c_state.data)\n",
    "        loss = criterion(prediction, y)\n",
    "\n",
    "        loss.backward()\n",
    "        for name, param in net.named_parameters():\n",
    "            print(f\"requires_grad: {param.requires_grad}\")\n",
    "            if param.grad is not None:\n",
    "                print(f\"Layer: {name} | Max Gradient: {param.grad.data.abs().max()}\")\n",
    "            else:\n",
    "                print(f\"Gradient is None\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 2:.7f}')\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        # Store predictions for plotting\n",
    "        prediction_plot.extend(prediction.cpu().detach().numpy())\n",
    "\n",
    "        # Detach states after updating to cut off the graph for the next batch\n",
    "        # h_state = h_state.detach()\n",
    "        # c_state = c_state.detach()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prediction_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "- Add SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are similar to that in training\n",
    "_, X_test_scaled = scale(X_train=X_train, X_val=X_test, type=scaler_type)\n",
    "test_dataloader = load_data(X=X_test, y=y_test, batch_size=batch_size, shuffle=shuffle)\n",
    "pred_dict, avg_test_loss = test_model.evaluate(test_dataloader=test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred_dict)\n",
    "pred_list = pred_dict['predicted']\n",
    "actual_list = pred_dict['actual']\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = root_mean_squared_error(y_true=actual_list, y_pred=pred_list)\n",
    "print(f\"RMSE: {rmse:.5f}, average test loss: {avg_test_loss:.5f} should be similar\") \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = range(len(pred_list))\n",
    "\n",
    "plt.plot(x, pred_list, label='Predicted')\n",
    "plt.plot(x, actual_list, label='Actual')\n",
    "plt.title('Actual vs Predicted for Test Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.model.eval()\n",
    "X_train_tensor = torch.tensor(X_train_scaled[:1000], dtype=torch.float32).to(DEVICE) # FIXME: Edit size\n",
    "X_test_tensor = torch.tensor(X_test_scaled[:5], dtype=torch.float32).to(DEVICE)\n",
    "# Need to use DeepExplainer for PyTorch, instead of Explainer\n",
    "# Analyse how each feature contributes to the prediction\n",
    "deepExplainer = shap.DeepExplainer(model=test_model.model, data=X_train_tensor)\n",
    "# Approximate shap values for the prediction\n",
    "shap_values = deepExplainer.shap_values(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed = np.squeeze(shap_values, axis=2)\n",
    "shap_values_squeezed.shape\n",
    "shap.summary_plot(shap_values_squeezed, X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numpy = X_test_tensor.cpu().numpy()\n",
    "X_test_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "# deepExplainer.expected_value = deepExplainer.expected_value[0]\n",
    "shap.plots.force(deepExplainer.expected_value, shap_values_squeezed, X_test_numpy, matplotlib=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards for really testing against test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X.iloc[:split_idx], y.iloc[:split_idx], shuffle=False, test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
