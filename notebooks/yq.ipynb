{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Finance\n",
    "import mplfinance as mpf\n",
    "import ta\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "# System\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('../') # Change the python path at runtime\n",
    "from src.utils import path as path_yq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "cur_dir = Path.cwd()\n",
    "\n",
    "POLYGON_API_KEY = os.environ.get(\"POLYGON_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Tick Data\n",
    "- Add feature to pull from data instead of fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale this up to n years\n",
    "ticker = \"NVDA\"\n",
    "max_limit = 50000\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "api_url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?adjusted=true&sort=asc&limit={max_limit}&apiKey={POLYGON_API_KEY}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"N\" # Reset\n",
    "str = input(\"Confirm?\")\n",
    "\n",
    "if str == \"Y\":\n",
    "    resp = requests.get(api_url)\n",
    "    print(f\"Request made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"VWAP\", \"Transactions\"]\n",
    "if resp.status_code == 200:\n",
    "    dict_list = resp.json().get('results')\n",
    "    df = pd.DataFrame(dict_list)\n",
    "\n",
    "    column_map = {\n",
    "        't': 'Timestamp',\n",
    "        'o': 'Open',\n",
    "        'h': 'High',\n",
    "        'l': 'Low',\n",
    "        'c': 'Close', \n",
    "        'n': 'Transactions', # Number of trades (market activity)\n",
    "        'v': 'Volume', # Number of shares traded (intensity of the activity)\n",
    "        'vw': 'VWAP'\n",
    "    }\n",
    "\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    df['Datetime'] = pd.to_datetime(df['Timestamp'], unit='ms')\n",
    "    df['Date'] = df['Datetime'].dt.normalize() # Remove the time and return date object\n",
    "\n",
    "    df = df[cols]\n",
    "    df.set_index(keys=\"Date\", inplace=True)\n",
    "else:\n",
    "    print(f\"Error fetching data: {resp.status_code}, {resp.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "historical_start_date = '2022-08-09'\n",
    "# Define the ticker list\n",
    "# Capitaland A17U, SUNT.SI cannot download\n",
    "ticker_list = ['S51.SI']\n",
    "\n",
    "# Fetch the data\n",
    "data = yf.download(ticker_list, historical_start_date)['Adj Close'] # Auto adjust is false\n",
    "data.index = pd.to_datetime(data.index)\n",
    "display(data.tail(20))\n",
    "data.plot()\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "sns.heatmap(data.corr(), cmap=\"Reds\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = path_yq.get_root_dir(cur_dir=cur_dir)\n",
    "csv_path = Path.joinpath(root_dir, \"data\", f\"{ticker}_{start_date}_{end_date}.csv\")\n",
    "\n",
    "# Get df\n",
    "try:\n",
    "    df.to_csv(csv_path)\n",
    "except NameError:\n",
    "    print(f\"df not defined, trying to fetch from csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index(keys=\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpf plot: https://github.com/matplotlib/mplfinance?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpf.plot(df, type='candle', style='charles', figsize=(20, 10), title=\"OHLC Bars for NVDA\", volume=True, show_nontrading=True, mav=(3, 6, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "- Technical indicators\n",
    "- Fundamental indicators\n",
    "- Date features\n",
    "- Holiday indicators etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- Train-test split\n",
    "- Scale data (only fit-transform for train, but not for test)\n",
    "- Decide which to predict. Have open and predict the close for the same day? Have the close for the previous day and predict next open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_indicators(tmp: pd.DataFrame):\n",
    "    df = tmp.copy(deep=True)\n",
    "\n",
    "    # Date features\n",
    "    df['Year'], df['Month'], df['Day'] = df.index.year, df.index.month, df.index.day\n",
    "\n",
    "    # Use the close price to create the indicators\n",
    "    df['BB High'], df['BB Low']= ta.volatility.bollinger_hband(df['Close']), ta.volatility.bollinger_lband(df['Close'])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Takes the past 20 data, including the current one to calculate the mean\n",
    "    df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
    "\n",
    "    df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "    df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "\n",
    "    df['Future Close'] = df['Close'].shift(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = create_technical_indicators(tmp=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "- Need to have train, val, test\n",
    "- General requirements:\n",
    "    - Ideally no NA values, otherwise model performance might be affected\n",
    "    - Number of rows TBC, cannot be too little (not enough data to train), and cannot \n",
    "    be too much also (computationally expensive)\n",
    "- Specific requirements:\n",
    "    - df2 (not sure if need date as index yet)\n",
    "    - Features (should be able to have both categorical and numerical)\n",
    "    - Target variable: Future Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need the current Close to predict the next close\n",
    "X = df2.drop(columns=['Future Close'])\n",
    "y = df2['Future Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "split_idx = int(len(df2) * 0.8)\n",
    "\n",
    "# FIXME: Assess the model on different sets and use the best one\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# TODO: Assess the models\n",
    "# Now do CV for assessing robustness, but later on, the train and validation should\n",
    "# Choose the best model/just go according to the sequential split\n",
    "for train_idx, val_idx in tscv.split(X.iloc[:split_idx]):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Scale TODO: Make into a function\n",
    "    scaler_choice = 'standard'\n",
    "    if scaler_choice == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    train_dataloader = load_data(X=X_train_scaled, y=y_train, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = load_data(X=X_val_scaled, y=y_val, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "X_test, y_test = X.iloc[split_idx:], y.iloc[split_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time is {elapsed_time:3f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') # torch.device('cpu')\n",
    "# assert mps_device == 'mps'\n",
    "print(device)\n",
    "batch_size = 64\n",
    "shuffle = False\n",
    "\n",
    "def convert_numpy_torch(arr: np.ndarray) -> torch:\n",
    "    if isinstance(arr, (pd.DataFrame, pd.Series)):\n",
    "        arr = arr.to_numpy()\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        # This does not make a copy, but any changes will affect the original arr\n",
    "        # Alternative: torch.tensor(arr, dtype=torch.float32)\n",
    "        return torch.from_numpy(arr).float().to(device) # Ensure it is a float tensor, and move to device\n",
    "    else:\n",
    "        raise ValueError(f\"The input\\n{arr}\\nis not an ndarray, it is a {type(arr)}.\")\n",
    "\n",
    "def load_data(X, y, batch_size, shuffle):\n",
    "    dataset = TensorDataset(convert_numpy_torch(X),\n",
    "                                convert_numpy_torch(y))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata = iter(train_dataloader)\n",
    "input, label = next(iterdata)\n",
    "input.shape\n",
    "print(len(train_dataloader), input.shape)\n",
    "# This is len_seq, batch_size, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3):\n",
    "        self.min_loss = np.inf\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "\n",
    "    def stop(self, loss: float) -> bool:\n",
    "        if loss < self.min_loss:\n",
    "            self.min_loss = loss\n",
    "            # Reset counter\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__() # Need bracket for super\n",
    "        self.hidden_size = kwargs.get('hidden_size', 1)\n",
    "        self.output_size = 1\n",
    "        self.bidir = False # Hardcoded since bidir won't be used for stock price pred\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=X.shape[-1], \n",
    "            hidden_size=self.hidden_size, \n",
    "            num_layers=1, # Dropout has no effect on single hidden layer\n",
    "            nonlinearity='tanh', # or relu\n",
    "            bias=True,\n",
    "            batch_first=False,\n",
    "            dropout=0.2,\n",
    "            bidirectional=self.bidir, # Can be True for NLP, but will introduce lookahead bias for stock\n",
    "            # device=kwargs.get('device', torch.device('cpu'))\n",
    "        )\n",
    "\n",
    "        total_features = self.hidden_size * 2 if self.bidir else self.hidden_size\n",
    "        # Use linear layer for fully connected layer to map to 1 column of output\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=total_features,\n",
    "            out_features=self.output_size\n",
    "            )\n",
    "    \n",
    "    # Must override the parent class's forward method\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "\n",
    "        print(rnn_out.shape) # [batch_size, features]\n",
    "\n",
    "        return self.fc(rnn_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tweak dropout, bidirectional, etc.\n",
    "lr = 0.001\n",
    "n_epoch = 50\n",
    "\n",
    "model = RNN(\n",
    "    hidden_size=4\n",
    "    ).to(device)\n",
    "optimiser = Adam(params=model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "loss_dict = defaultdict(list)\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for input, target in train_dataloader:\n",
    "        optimiser.zero_grad() # Resets gradient of the optimised Tensors to None\n",
    "        output = model(input)\n",
    "        print(output.shape, target.shape)\n",
    "        assert output.squeeze(-1).shape == target.shape\n",
    "        loss = loss_fn(output, target)\n",
    "        total_train_loss += loss.item() # Tensor operation: get the scalar in a tensor with 1 element\n",
    "\n",
    "        loss.backward() # Compute the gradient of the loss wrt weights, backpropagate\n",
    "        optimiser.step() # Takes a step in the direction that reduces the loss, updates params\n",
    "    model.eval() # Disable dropout\n",
    "    with torch.no_grad():\n",
    "        for input, target in val_dataloader:\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    # Add train, val loss to dict for each epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) # Take the mean of MSE for all batches\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    loss_dict['train_loss'].append(avg_train_loss)\n",
    "    loss_dict['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    if early_stopper.stop(loss=avg_val_loss) == True:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1} has completed.\")\n",
    "\n",
    "# After the whole training is completed, we can plot the losses, time and analyse which model is the best \n",
    "# Train with full set and save the model for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards for really testing against test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X.iloc[:split_idx], y.iloc[:split_idx], shuffle=False, test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
